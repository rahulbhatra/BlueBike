```{r}
tripData = read.csv("TripData/2019-tripdata.csv")
tripData
```

```{r}
tripData$startDate = as.Date(tripData$starttime, "%Y-%m-%d")
tripData$stopDate = as.Date(tripData$stoptime, "%Y-%m-%d")
library(dplyr)
numberOfTrips = tripData %>% group_by(startDate) %>% 
  summarise(totalTrips=length(bikeid),
            .groups = 'drop')
numberOfTrips
plot(x = numberOfTrips$startDate, y = numberOfTrips$totalTrips, xlab = "Month of the Year", ylab = "Total Number of Trips", type = "l", main = "2019 Date Vs Number of Trips")
```


```{r}
crimeData = read.csv("CrimeData/crimeData.csv")
crimeData
```

```{r}
library(dplyr)
crimeData$OCCURRED_ON_DATE = as.Date(crimeData$OCCURRED_ON_DATE, "%Y-%m-%d")
crimeCount = crimeData %>% group_by(OCCURRED_ON_DATE) %>% 
  summarise(totalCrimes=length(INCIDENT_NUMBER),
            .groups = 'drop')
crimeData
crimeCount = crimeCount[crimeCount$OCCURRED_ON_DATE >= as.Date("2019-01-01") & crimeCount$OCCURRED_ON_DATE <= as.Date("2019-12-31"), ]
length(crimeCount$totalCrimes)
length(crimeCount$OCCURRED_ON_DATE)
plot(x = crimeCount$OCCURRED_ON_DATE, y = crimeCount$totalCrimes, xlab = "Month of the Year", ylab = "Total Number of Crimes", type = "l", main = "2019 Date Vs Number of Crimes")
```

Combining the data
```{r}
weatherData = read.csv("WeatherData/Weather2019.csv")
weatherData$datetime = as.Date(weatherData$datetime, "%Y-%m-%d")
weatherData
```


```{r}
data = merge(numberOfTrips, weatherData, by.x = "startDate", by.y ="datetime")
data = merge(data, crimeCount, by.x = "startDate", by.y = "OCCURRED_ON_DATE")
data
```


```{r}
str(data)
library("ggplot2")
ggp = ggplot(data = data) +
  geom_line(aes(x = startDate, y = totalTrips), color = 2) + 
ggp

ggplot(data = data) + 
  geom_line(aes(x = startDate, y = totalCrimes), color = 3) +
  geom_line(aes(x = startDate, y = temp), color = 4)
```

```{r}
library(keras)
library(tensorflow)

head(data)
scale_factors = c(mean(data$totalTrips), sd(data$totalTrips))
scale_factors
```


```{r}
scaled_train <- data %>%
    dplyr::select(totalTrips) %>%
    dplyr::mutate(totalTrips = (totalTrips - scale_factors[1]) / scale_factors[2])

scaled_train = as.matrix(scaled_train)
scaled_train

prediction <- 12
lag <- prediction

x_train_data <- t(sapply(
    1:(length(scaled_train) - lag - prediction + 1),
    function(x) scaled_train[x:(x + lag - 1), 1]
  ))

x_train_data

# now we transform it into 3D form
x_train_arr <- array(
    data = as.numeric(unlist(x_train_data)),
    dim = c(
        nrow(x_train_data),
        lag,
        1
    )
)

x_train_arr


y_train_data <- t(sapply(
    (1 + lag):(length(scaled_train) - prediction + 1),
    function(x) scaled_train[x:(x + prediction - 1)]
))
y_train_data
 
y_train_arr <- array(
    data = as.numeric(unlist(y_train_data)),
    dim = c(
        nrow(y_train_data),
        prediction,
        1
    )
)
y_train_arr
```


```{r}
x_test <- data$totalTrips[(nrow(scaled_train) - prediction + 1):nrow(scaled_train)]
x_test

x_test_scaled <- (x_test - scale_factors[1]) / scale_factors[2]
 
# this time our array just has one sample, as we intend to perform one 12-months prediction
x_pred_arr <- array(
    data = x_test_scaled,
    dim = c(
        1,
        lag,
        1
    )
)
```


```{r}
lstm_model <- keras_model_sequential()
 
lstm_model %>%
  layer_lstm(units = 50, # size of the layer
       batch_input_shape = c(1, 12, 1), # batch size, timesteps, features
       return_sequences = TRUE,
       stateful = TRUE) %>%
  # fraction of the units to drop for the linear transformation of the inputs
  layer_dropout(rate = 0.5) %>%
  layer_lstm(units = 50,
        return_sequences = TRUE,
        stateful = TRUE) %>%
  layer_dropout(rate = 0.5) %>%
  time_distributed(keras::layer_dense(units = 1))
```

```{r}
lstm_model %>%
    compile(loss = 'mae', optimizer = 'adam', metrics = 'accuracy')
 
summary(lstm_model)
```


```{r}
lstm_model %>% fit(
    x = x_train_arr,
    y = y_train_arr,
    batch_size = 1,
    epochs = 20,
    verbose = 0,
    shuffle = FALSE
)
```


```{r}
lstm_forecast <- lstm_model %>%
    predict(x_pred_arr, batch_size = 1) %>%
    .[, , 1]
 
# we need to rescale the data to restore the original values
lstm_forecast <- lstm_forecast * scale_factors[2] + scale_factors[1]


fitted <- predict(lstm_model, x_train_arr, batch_size = 1) %>%
   .[, , 1]
fitted

if (dim(fitted)[2] > 1) {
    fit <- c(fitted[, 1], fitted[dim(fitted)[1], 2:dim(fitted)[2]])
} else {
    fit <- fitted[, 1]
}
 
# additionally we need to rescale the data
fitted <- fit * scale_factors[2] + scale_factors[1]
fitted
length(fitted)

# I specify first forecast values as not available
fitted <- c(rep(NA, lag), fitted)

lstm_forecast <- timetk::tk_ts(lstm_forecast,
    start = c(2020, 1, 1),
    end = c(2020, 12, 31),
    frequency = 12
)
lstm_forecast
```

```{r}
input_ts <- timetk::tk_ts(data$totalTrips, 
    start = c(2019, 1),
    end = c(2019, 12),
    frequency = 12)
input_ts
```

```{r}

forecast_list <- list(
    model = NULL,
    method = "LSTM",
    mean = lstm_forecast,
    x = input_ts,
    fitted = fitted,
    residuals = as.numeric(input_ts) - as.numeric(fitted)
  )
 
class(forecast_list) <- "forecast"

forecast::autoplot(forecast_list)
```

